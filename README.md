# etl-project


The project task was to build a batch ETL pipeline -  

  to ingest transactional data from RDS into HDFS (using AWS EC2) via Sqoop; 
  to transform the data using PySpark (using AWS EC2) to create relevant dimension and fact tables (Data Mart) 
  to upload these tables into AWS S3 buckets
  load them from S3 into AWS Redshift tables.


